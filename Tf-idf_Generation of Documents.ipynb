{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Assignment1_Optimized.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBGXnNFoDUMM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "outputId": "b9c61208-9e1f-4fdc-bc41-f94383bbb20d"
      },
      "source": [
        "!pip install pyunpack\n",
        "!pip install patool\n",
        "from pyunpack import Archive\n",
        "Archive('/content/drive/My Drive/MSDS/IR_Assignment/ACL txt.rar').extractall('/content/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyunpack\n",
            "  Downloading https://files.pythonhosted.org/packages/83/29/020436b1d8e96e5f26fa282b9c3c13a3b456a36b9ea2edc87c5fed008369/pyunpack-0.2.2-py2.py3-none-any.whl\n",
            "Collecting entrypoint2\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/ca/00c8767568924e5c2209da99b6abdeeed9d11cbae2a713d54d041b092a09/entrypoint2-0.2.3-py2.py3-none-any.whl\n",
            "Collecting easyprocess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Collecting argparse\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Installing collected packages: argparse, entrypoint2, easyprocess, pyunpack\n",
            "Successfully installed argparse-1.4.0 easyprocess-0.3 entrypoint2-0.2.3 pyunpack-0.2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting patool\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-51e2bda6b9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install patool'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyunpack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArchive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mArchive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/MSDS/IR_Assignment/ACL txt.rar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyunpack/__init__.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, directory, auto_create_dir, patool_path)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fullpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"archive file does not exist:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mauto_create_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: archive file does not exist:/content/drive/My Drive/MSDS/IR_Assignment/ACL txt.rar"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyvbjjhLaGov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce837d66-4227-4d2c-f3fd-fc6c60ad3138"
      },
      "source": [
        "import nltk\n",
        "import os\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "stopwords=stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY7FJCd9_HmN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5903e97d-c450-4808-c1fc-40116f5a1110"
      },
      "source": [
        "query_list=[\"LDA\",\"Topic modelling\",\"Generative models\",\"Semantic relationships between terms\",\"Natural Language Processing\",\"Text Mining\",\"Translation model\",\"Learning procedures for the lexicon\",\"Semantic evaluations\",\"System results and combination\"]\n",
        "path = \"/content/ACL txt/\"\n",
        "Files_List = listdir(\"/content/ACL txt/\")\n",
        "tf_idf_doc=tfidf(Files_List)\n",
        "for query in query_list:\n",
        "   print(query,\"\\n\",vectoreSpaceModel(query,tf_idf_doc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LDA \n",
            " {'P14-1129.pdf.txt': 0.0, 'P14-2001.pdf.txt': 0, 'W11-0208.pdf.txt': 0, 'W00-1300.pdf.txt': 0, 'N15-2022.pdf.txt': 0}\n",
            "Topic modelling \n",
            " {'W14-4724.pdf.txt': 88.91118742299811, 'W97-1007.pdf.txt': 67.57250244147856, 'P15-2085.pdf.txt': 67.57250244147856, 'S13-1001.pdf.txt': 64.01605494455863, 'W01-1405.pdf.txt': 56.90315995071879}\n",
            "Generative models \n",
            " {'W11-0100.pdf.txt': 135.38213517963428, 'J03-1002.pdf.txt': 100.96294826955776, 'J03-4003.pdf.txt': 99.4332066291099, 'J15-4004.pdf.txt': 94.07911088754246, 'J03-3003.pdf.txt': 84.90066104485538}\n",
            "Semantic relationships between terms \n",
            " {'W11-0100.pdf.txt': 244.53612729192315, 'W04-1801.pdf.txt': 141.2286064044653, 'W15-3808.pdf.txt': 138.92259477728976, 'J86-3001.pdf.txt': 132.74104578343056, 'N03-1035.pdf.txt': 131.79748775102948}\n",
            "Natural Language Processing \n",
            " {'P14-1129.pdf.txt': 0.0, 'P14-2001.pdf.txt': 0.0, 'W11-0208.pdf.txt': 0.0, 'W00-1300.pdf.txt': 0.0, 'N15-2022.pdf.txt': 0.0}\n",
            "Text Mining \n",
            " {'P14-1129.pdf.txt': 0, 'P14-2001.pdf.txt': 0.0, 'W11-0208.pdf.txt': 0.0, 'W00-1300.pdf.txt': 0.0, 'N15-2022.pdf.txt': 0.0}\n",
            "Translation model \n",
            " {'W11-0100.pdf.txt': 170.36494003805154, 'J03-2004.pdf.txt': 97.89985286693667, 'J93-2003.pdf.txt': 79.66360576427199, 'J10-4007.pdf.txt': 77.26409956655296, 'J09-3001.pdf.txt': 75.82439584792154}\n",
            "Learning procedures for the lexicon \n",
            " {'W11-0100.pdf.txt': 478.8163128419395, 'J87-3007.pdf.txt': 225.08629422553605, 'W10-2505.pdf.txt': 194.77221074566214, 'W14-55.x.pdf.txt': 175.8911696574933, 'W06-1647.pdf.txt': 160.44656944112768}\n",
            "Semantic evaluations \n",
            " {'J09-4008.pdf.txt': 171.84701628507793, 'J13-2002.pdf.txt': 103.10820977104676, 'D09-1032.pdf.txt': 68.73880651403117, 'P10-1059.pdf.txt': 59.36533289848147, 'J01-2003.pdf.txt': 56.240841693298236}\n",
            "System results and combination \n",
            " {'W11-0100.pdf.txt': 277.21417344011877, 'J01-2002.pdf.txt': 134.66512674649584, 'N10-1141.pdf.txt': 118.8770053488992, 'J15-2004.pdf.txt': 109.86160493406035, 'P11-1127.pdf.txt': 95.38881633118174}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnswsslWx_21"
      },
      "source": [
        "def vectoreSpaceModel(query,tf_idf_doc):\n",
        "  # print(Files_List)\n",
        "  query_split= querysplit(query)\n",
        "  query_wc= query_vocab(query,query_split)\n",
        "  final_score=score(query_split,query_wc,tf_idf_doc)\n",
        "  # print(\"This is score\",final_score)\n",
        "  Rank={}\n",
        "  Rank = {Files_List[i]: final_score[i] for i in range(len(Files_List))}\n",
        "  # print(Rank)\n",
        "  Top5= dict(Counter(Rank).most_common(5))\n",
        "  return Top5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5WqyCJZx1iD"
      },
      "source": [
        "def tfidf(docList):\n",
        "  length=len(docList)\n",
        "  List =[]\n",
        "  for i in docList: \n",
        "      doc=path+i\n",
        "      wordList=WordList(doc)\n",
        "      List.append(termFrequencyInDoc(wordList))\n",
        "  # print(\"this is list with term\",List)\n",
        "  dicList=wordDocFre(List)\n",
        "  # print(\"this is dic list\",dicList)\n",
        "  idf=inverseDocFre(length,dicList)\n",
        "  # print(\"this is idf\",idf)\n",
        "  tf_idf_doc=tfidfdoc(List,idf)\n",
        "  # print(\"this is tfidf\",tf_idf_doc)\n",
        "  return tf_idf_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPrt-qJh4fGs"
      },
      "source": [
        "def wordDocFre(List_TF):\n",
        "  Doc_cor={}\n",
        "  for sub in List_TF: \n",
        "     for word in sub.keys():\n",
        "          if word not in Doc_cor.keys():\n",
        "            Doc_cor[word]=1\n",
        "          else:\n",
        "            Doc_cor[word]=Doc_cor[word]+1\n",
        "  return Doc_cor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5___opIOXGo"
      },
      "source": [
        "def tfidfdoc(Listf,idf):\n",
        "  doc=[]\n",
        "  for dic in Listf:\n",
        "     doc.append(dic)\n",
        "  for dic in doc:\n",
        "    for word in dic.keys():\n",
        "      dic[word]=dic[word]*idf[word]\n",
        "  return doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQeuQZ7gQFdw"
      },
      "source": [
        "def score(query,query_wc,tfidf):\n",
        "  rel_score=[]\n",
        "  # for dic in tfidf:\n",
        "  #    rel_score.append({})\n",
        "  for dic in tfidf:\n",
        "    for word in dic.keys():\n",
        "        score=0\n",
        "        for qword in query:\n",
        "            if qword in dic.keys():\n",
        "              score += query_wc[qword]*dic[qword]\n",
        "    rel_score.append(score)\n",
        "\n",
        "  return rel_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-jOvwfhL431"
      },
      "source": [
        "def inverseDocFre(length,dicList):\n",
        "  docfre = {} \n",
        "  M = length\n",
        "  for word in dicList:\n",
        "      docfre[word] = np.log2((M+1) / dicList[word]) #log_2 ((M+1)/k) i.e inverse document frequency\n",
        "  return docfre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSZudfyIdWcs"
      },
      "source": [
        "def termFrequencyInDoc(wordList):\n",
        "  termfre = {} # a dictionary to store count of a word in the query (i.e x_i according to lecture slides terminology)\n",
        "  for word in wordList:\n",
        "      termfre[word] = wordList.count(word)\n",
        "  return termfre"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_jXZL2EaT2H"
      },
      "source": [
        "def WordList(doc):\n",
        "  file=open(doc,encoding='latin1')\n",
        "  wordList=file.read().split()\n",
        "  return removePuncs(wordList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPLdObj4aVbX"
      },
      "source": [
        "def removePuncs(wordList):\n",
        "   words=[w.lower() for w in wordList if w.isalpha()]\n",
        "   return[word for word in words if word not in stopwords]\n",
        "   #return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1URA5z37V_p"
      },
      "source": [
        "def querysplit(query):\n",
        "  query_split=[]\n",
        "  query_split=query.split()\n",
        "  return removePuncs(query_split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mubOmoHn7-0l"
      },
      "source": [
        "def query_vocab(query,query_split):\n",
        "  query_wc = {} # a dictionary to store count of a word in the query (i.e x_i according to lecture slides terminology)\n",
        "  for word in query_split:\n",
        "    query_wc[word] = query.split().count(word)\n",
        "  return query_wc"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}